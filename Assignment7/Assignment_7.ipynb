{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 7b.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYiRsFGD6iUC"
      },
      "source": [
        "# 0 TorchText"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp5IzBGsPGHs"
      },
      "source": [
        "## Dataset Preview\n",
        "\n",
        "Your first step to deep learning in NLP. We will be mostly using PyTorch. Just like torchvision, PyTorch provides an official library, torchtext, for handling text-processing pipelines. \n",
        "\n",
        "We will be using previous session tweet dataset. Let's just preview the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKLIDmto9-Jd",
        "outputId": "74d97a14-c24f-4035-bdf5-ae244074491c"
      },
      "source": [
        "!pip install google_trans_new"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: google_trans_new in /usr/local/lib/python3.6/dist-packages (1.1.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PW8bFvGS9-3F",
        "outputId": "0124af0d-cdd5-42ab-bf19-e8543d10fab5"
      },
      "source": [
        "!pip install PyDictionary "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyDictionary in /usr/local/lib/python3.6/dist-packages (2.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from PyDictionary) (2.23.0)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (from PyDictionary) (0.0.1)\n",
            "Requirement already satisfied: goslate in /usr/local/lib/python3.6/dist-packages (from PyDictionary) (1.5.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from PyDictionary) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->PyDictionary) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->PyDictionary) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->PyDictionary) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->PyDictionary) (1.24.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4->PyDictionary) (4.6.3)\n",
            "Requirement already satisfied: futures in /usr/local/lib/python3.6/dist-packages (from goslate->PyDictionary) (3.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpjUGANrpnrE"
      },
      "source": [
        "#CUDA_LAUNCH_BLOCKING=1\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings(\"ignore\")\r\n",
        "import spacy\r\n",
        "nlp = spacy.load('en')\r\n",
        "import random\r\n",
        "from nltk.corpus import wordnet\r\n",
        "from PyDictionary import PyDictionary"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1-Yz-5RRFYc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "9e8b9e5f-d56a-4e48-b69b-2efa124acaee"
      },
      "source": [
        "import pandas as pd\n",
        "df1 = pd.read_csv('datasetSentences.txt', sep = '\\t')\n",
        "df1.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_index</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Effective but too-tepid biopic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>If you sometimes like to go to the movies to h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Emerges as something rare , an issue movie tha...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentence_index                                           sentence\n",
              "0               1  The Rock is destined to be the 21st Century 's...\n",
              "1               2  The gorgeously elaborate continuation of `` Th...\n",
              "2               3                     Effective but too-tepid biopic\n",
              "3               4  If you sometimes like to go to the movies to h...\n",
              "4               5  Emerges as something rare , an issue movie tha..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "R1ZKCjrFjzoT",
        "outputId": "ede1bcbe-67ce-422c-ab7f-d5c5f22e8cc6"
      },
      "source": [
        "df2 = pd.read_csv('datasetSplit.txt', sep = ',')\r\n",
        "df2.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_index</th>\n",
              "      <th>splitset_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentence_index  splitset_label\n",
              "0               1               1\n",
              "1               2               1\n",
              "2               3               2\n",
              "3               4               2\n",
              "4               5               2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "usLrwt6rj_b-",
        "outputId": "9231c1d9-477e-442d-9c3c-2e0171e1154c"
      },
      "source": [
        "df3 = pd.read_csv('sentiment_labels.txt', sep = '|')\r\n",
        "df3.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>phrase ids</th>\n",
              "      <th>sentiment values</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.50000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.50000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.44444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.50000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.42708</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   phrase ids  sentiment values\n",
              "0           0           0.50000\n",
              "1           1           0.50000\n",
              "2           2           0.44444\n",
              "3           3           0.50000\n",
              "4           4           0.42708"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "zDJcr-8dkOLQ",
        "outputId": "e2cd4027-a062-4ed1-a7df-9b94e9b74544"
      },
      "source": [
        "df3['labels'] = 0\r\n",
        "\r\n",
        "v_range = 5\r\n",
        "\r\n",
        "for i in range(v_range):\r\n",
        "  df3['labels'][(df3['sentiment values'] > i*(1/v_range)) & (df3['sentiment values'] <= (i+1)*(1/v_range))] = i\r\n",
        "\r\n",
        "df3.head(10)\r\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>phrase ids</th>\n",
              "      <th>sentiment values</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.50000</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.50000</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.44444</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.50000</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.42708</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>0.37500</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>0.41667</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>0.54167</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>0.33333</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>0.45833</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   phrase ids  sentiment values  labels\n",
              "0           0           0.50000       2\n",
              "1           1           0.50000       2\n",
              "2           2           0.44444       2\n",
              "3           3           0.50000       2\n",
              "4           4           0.42708       2\n",
              "5           5           0.37500       1\n",
              "6           6           0.41667       2\n",
              "7           7           0.54167       2\n",
              "8           8           0.33333       1\n",
              "9           9           0.45833       2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "92WSlJh1llF9",
        "outputId": "414de450-c635-4833-86b0-c2a3915a2320"
      },
      "source": [
        "df4 = df1.merge(df2, left_on='sentence_index', right_on='sentence_index')\r\n",
        "df = df4.merge(df3, left_on='sentence_index', right_on='phrase ids')\r\n",
        "\r\n",
        "df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_index</th>\n",
              "      <th>sentence</th>\n",
              "      <th>splitset_label</th>\n",
              "      <th>phrase ids</th>\n",
              "      <th>sentiment values</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.50000</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.44444</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Effective but too-tepid biopic</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0.50000</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>If you sometimes like to go to the movies to h...</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0.42708</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Emerges as something rare , an issue movie tha...</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>0.37500</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentence_index  ... labels\n",
              "0               1  ...      2\n",
              "1               2  ...      2\n",
              "2               3  ...      2\n",
              "3               4  ...      2\n",
              "4               5  ...      1\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "dOEQP2tkmm68",
        "outputId": "d4fb3f6d-c0be-4fbd-caf4-9a2be4325690"
      },
      "source": [
        "df[df['sentence_index'] != df['phrase ids']]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_index</th>\n",
              "      <th>sentence</th>\n",
              "      <th>splitset_label</th>\n",
              "      <th>phrase ids</th>\n",
              "      <th>sentiment values</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [sentence_index, sentence, splitset_label, phrase ids, sentiment values, labels]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7JdpCW-YbAG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35310db9-c9ca-4da5-d4bf-56164d7eaf47"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11855, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqRsoF6xYdgl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "799e37ed-31fb-4a1b-f2b4-95294a5dae74"
      },
      "source": [
        "df.labels.value_counts()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    6240\n",
              "1    2175\n",
              "3    2139\n",
              "4     663\n",
              "0     638\n",
              "Name: labels, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpLSHRhNDhD6"
      },
      "source": [
        "df_train =  df[df['splitset_label'] == 1]\r\n",
        "df_valid =  df[df['splitset_label'] == 3]\r\n",
        "df_test =  df[df['splitset_label'] == 2]\r\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lm2tQf08DAll"
      },
      "source": [
        "df_train = df_train.reset_index(drop = True)\r\n",
        "df_valid = df_valid.reset_index(drop = True)\r\n",
        "df_test = df_test.reset_index(drop = True)\r\n",
        "df_temp = df_train.copy()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqu6qk5MC-nB"
      },
      "source": [
        "my_stopwords = nlp.Defaults.stop_words\r\n",
        "dictionary=PyDictionary()\r\n",
        "\r\n",
        "def get_synonyms(word):\r\n",
        "  punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\r\n",
        "  if word in punctuations:\r\n",
        "    return word\r\n",
        "  v_synonym = dictionary.synonym(word)\r\n",
        "  \r\n",
        "  if (v_synonym is not None):\r\n",
        "    return v_synonym[0]\r\n",
        "  else:\r\n",
        "    return word\r\n",
        "\r\n",
        "def remove_stopwords(sentence):\r\n",
        "    tokens = sentence.split(\" \")\r\n",
        "    tokens_filtered= [word for word in tokens if not word in my_stopwords]\r\n",
        "    return (\" \").join(tokens_filtered)\r\n",
        "\r\n",
        "def random_insertion(sentence, n): \r\n",
        "    words = remove_stopwords(sentence) \r\n",
        "    for _ in range(n):\r\n",
        "        tokens = words.split(\" \")\r\n",
        "        temp = random.choice(tokens)\r\n",
        "        new_synonym = get_synonyms(temp)\r\n",
        "        sentence = sentence.replace(temp, new_synonym)\r\n",
        "    return sentence"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbGE3ZhDDUQl",
        "outputId": "cb53d0c3-70c0-45a1-bf41-4e45b4b31e45"
      },
      "source": [
        "v_len = df_temp.shape[0]\r\n",
        "\r\n",
        "for i in range(int(v_len/10)):\r\n",
        "  row = random.randint(0, v_len)\r\n",
        "  df_temp['sentence'][row] = random_insertion(df_temp['sentence'][row], 5)\r\n",
        "  if (i%100 == 0):\r\n",
        "    print(i)\r\n",
        "  \r\n",
        "\r\n",
        "#df_train1 = df_train.append(df_temp[0])\r\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'' has no Synonyms in the API\n",
            "0\n",
            "... has no Synonyms in the API\n",
            "Vs. has no Synonyms in the API\n",
            "charmless has no Synonyms in the API\n",
            "Gallo has no Synonyms in the API\n",
            "Vincent has no Synonyms in the API\n",
            "Chances has no Synonyms in the API\n",
            "Chances has no Synonyms in the API\n",
            "... has no Synonyms in the API\n",
            "Kids has no Synonyms in the API\n",
            "inexplicably has no Synonyms in the API\n",
            "inexplicably has no Synonyms in the API\n",
            "watchable has no Synonyms in the API\n",
            "Jolie has no Synonyms in the API\n",
            "`` has no Synonyms in the API\n",
            "'' has no Synonyms in the API\n",
            "Caviezel has no Synonyms in the API\n",
            "Caviezel has no Synonyms in the API\n",
            "Caviezel has no Synonyms in the API\n",
            "WWII has no Synonyms in the API\n",
            "Hong has no Synonyms in the API\n",
            "Kong has no Synonyms in the API\n",
            "-- has no Synonyms in the API\n",
            "Iles has no Synonyms in the API\n",
            "What has no Synonyms in the API\n",
            "mentally has no Synonyms in the API\n",
            "Tinseltown has no Synonyms in the API\n",
            "Tinseltown has no Synonyms in the API\n",
            "Everything has no Synonyms in the API\n",
            "... has no Synonyms in the API\n",
            "Majidi has no Synonyms in the API\n",
            "Majidi has no Synonyms in the API\n",
            "Unsurprisingly has no Synonyms in the API\n",
            "... has no Synonyms in the API\n",
            "... has no Synonyms in the API\n",
            "... has no Synonyms in the API\n",
            "-- has no Synonyms in the API\n",
            "Went has no Synonyms in the API\n",
            "We has no Synonyms in the API\n",
            "Seinfeld has no Synonyms in the API\n",
            "Dan has no Synonyms in the API\n",
            "Meticulously has no Synonyms in the API\n",
            "emptily has no Synonyms in the API\n",
            "joylessly has no Synonyms in the API\n",
            "underwhelming has no Synonyms in the API\n",
            "clichÃ© has no Synonyms in the API\n",
            "Stevenon has no Synonyms in the API\n",
            "100\n",
            "Depicts has no Synonyms in the API\n",
            "Charlie has no Synonyms in the API\n",
            "watchable has no Synonyms in the API\n",
            "CQ has no Synonyms in the API\n",
            "` has no Synonyms in the API\n",
            "` has no Synonyms in the API\n",
            "... has no Synonyms in the API\n",
            "Simone has no Synonyms in the API\n",
            "-- has no Synonyms in the API\n",
            "... has no Synonyms in the API\n",
            "... has no Synonyms in the API\n",
            "This has no Synonyms in the API\n",
            "Its has no Synonyms in the API\n",
            "... has no Synonyms in the API\n",
            "documentarians has no Synonyms in the API\n",
            "Frida has no Synonyms in the API\n",
            "Walsh has no Synonyms in the API\n",
            "`` has no Synonyms in the API\n",
            "Americans has no Synonyms in the API\n",
            "Americans has no Synonyms in the API\n",
            "shockwaves has no Synonyms in the API\n",
            "disposible has no Synonyms in the API\n",
            "disposible has no Synonyms in the API\n",
            "disposible has no Synonyms in the API\n",
            "Runteldat has no Synonyms in the API\n",
            "Runteldat has no Synonyms in the API\n",
            "If has no Synonyms in the API\n",
            "If has no Synonyms in the API\n",
            "-LRB- has no Synonyms in the API\n",
            "Jia has no Synonyms in the API\n",
            "Scorcese has no Synonyms in the API\n",
            "Gangs has no Synonyms in the API\n",
            "-- has no Synonyms in the API\n",
            "Their has no Synonyms in the API\n",
            "Their has no Synonyms in the API\n",
            "Their has no Synonyms in the API\n",
            "predictably has no Synonyms in the API\n",
            "` has no Synonyms in the API\n",
            "` has no Synonyms in the API\n",
            "` has no Synonyms in the API\n",
            "LaPaglia has no Synonyms in the API\n",
            "immaculately has no Synonyms in the API\n",
            "thematic has no Synonyms in the API\n",
            "Degenerates has no Synonyms in the API\n",
            "` has no Synonyms in the API\n",
            "` has no Synonyms in the API\n",
            "Schaeffer has no Synonyms in the API\n",
            "achronological has no Synonyms in the API\n",
            "achronological has no Synonyms in the API\n",
            "laugther has no Synonyms in the API\n",
            "effortlessly has no Synonyms in the API\n",
            "positively has no Synonyms in the API\n",
            "200\n",
            "` has no Synonyms in the API\n",
            "Phifer has no Synonyms in the API\n",
            "Affleck has no Synonyms in the API\n",
            "This has no Synonyms in the API\n",
            "This has no Synonyms in the API\n",
            "This has no Synonyms in the API\n",
            "gloriously has no Synonyms in the API\n",
            "gloriously has no Synonyms in the API\n",
            "Nicholson has no Synonyms in the API\n",
            "-LRB- has no Synonyms in the API\n",
            "Vera has no Synonyms in the API\n",
            "-- has no Synonyms in the API\n",
            "Captures has no Synonyms in the API\n",
            "'' has no Synonyms in the API\n",
            "`` has no Synonyms in the API\n",
            "This has no Synonyms in the API\n",
            "Niro has no Synonyms in the API\n",
            "Viterelli has no Synonyms in the API\n",
            "... has no Synonyms in the API\n",
            "Wendigo has no Synonyms in the API\n",
            "pretention has no Synonyms in the API\n",
            "Its has no Synonyms in the API\n",
            "psychodramatics has no Synonyms in the API\n",
            "Women has no Synonyms in the API\n",
            "-RRB- has no Synonyms in the API\n",
            "-- has no Synonyms in the API\n",
            "Is has no Synonyms in the API\n",
            "Ourside has no Synonyms in the API\n",
            "realistically has no Synonyms in the API\n",
            "realistically has no Synonyms in the API\n",
            "Pellington has no Synonyms in the API\n",
            "delightfully has no Synonyms in the API\n",
            "Curves has no Synonyms in the API\n",
            "Ontiveros has no Synonyms in the API\n",
            "Curves has no Synonyms in the API\n",
            "Fontaine has no Synonyms in the API\n",
            "watchable has no Synonyms in the API\n",
            "watchable has no Synonyms in the API\n",
            "... has no Synonyms in the API\n",
            "Bratt has no Synonyms in the API\n",
            "-LRB- has no Synonyms in the API\n",
            "This has no Synonyms in the API\n",
            "hipness has no Synonyms in the API\n",
            "Manages has no Synonyms in the API\n",
            "shrieky has no Synonyms in the API\n",
            "Quaid has no Synonyms in the API\n",
            "300\n",
            "Consequences has no Synonyms in the API\n",
            "Brothers has no Synonyms in the API\n",
            "Brothers has no Synonyms in the API\n",
            "This has no Synonyms in the API\n",
            "Hugh has no Synonyms in the API\n",
            "Sean has no Synonyms in the API\n",
            "Julia has no Synonyms in the API\n",
            "`` has no Synonyms in the API\n",
            "Terminally has no Synonyms in the API\n",
            "-RRB- has no Synonyms in the API\n",
            "... has no Synonyms in the API\n",
            "... has no Synonyms in the API\n",
            "What has no Synonyms in the API\n",
            "... has no Synonyms in the API\n",
            "... has no Synonyms in the API\n",
            "... has no Synonyms in the API\n",
            "... has no Synonyms in the API\n",
            "Knockaround has no Synonyms in the API\n",
            "Britney has no Synonyms in the API\n",
            "'' has no Synonyms in the API\n",
            "specifically has no Synonyms in the API\n",
            "-RRB- has no Synonyms in the API\n",
            "Oozes has no Synonyms in the API\n",
            "-- has no Synonyms in the API\n",
            "Although has no Synonyms in the API\n",
            "Jason has no Synonyms in the API\n",
            "Jason has no Synonyms in the API\n",
            "Damon has no Synonyms in the API\n",
            "Jason has no Synonyms in the API\n",
            "Maintains has no Synonyms in the API\n",
            "-- has no Synonyms in the API\n",
            "jokey has no Synonyms in the API\n",
            "Evelyn has no Synonyms in the API\n",
            "... has no Synonyms in the API\n",
            "drang has no Synonyms in the API\n",
            "Chris has no Synonyms in the API\n",
            "marveilleux has no Synonyms in the API\n",
            "-LRB- has no Synonyms in the API\n",
            "`` has no Synonyms in the API\n",
            "Feardotcom has no Synonyms in the API\n",
            "`` has no Synonyms in the API\n",
            "Watchable has no Synonyms in the API\n",
            "marginally has no Synonyms in the API\n",
            "... has no Synonyms in the API\n",
            "... has no Synonyms in the API\n",
            "... has no Synonyms in the API\n",
            "Horton has no Synonyms in the API\n",
            "Eric has no Synonyms in the API\n",
            "Ritchie has no Synonyms in the API\n",
            "400\n",
            "Irwin has no Synonyms in the API\n",
            "Affleck has no Synonyms in the API\n",
            "Affleck has no Synonyms in the API\n",
            "Sometimes has no Synonyms in the API\n",
            "Sometimes has no Synonyms in the API\n",
            "` has no Synonyms in the API\n",
            "This has no Synonyms in the API\n",
            "E.T. has no Synonyms in the API\n",
            "Avary has no Synonyms in the API\n",
            "Its has no Synonyms in the API\n",
            "Avary has no Synonyms in the API\n",
            "Avary has no Synonyms in the API\n",
            "-LRB- has no Synonyms in the API\n",
            "This has no Synonyms in the API\n",
            "-- has no Synonyms in the API\n",
            "Killed has no Synonyms in the API\n",
            "virulently has no Synonyms in the API\n",
            "` has no Synonyms in the API\n",
            "greatly has no Synonyms in the API\n",
            "generational has no Synonyms in the API\n",
            "Caddyshack has no Synonyms in the API\n",
            "Ballesta has no Synonyms in the API\n",
            "Harks has no Synonyms in the API\n",
            "Harks has no Synonyms in the API\n",
            "Harks has no Synonyms in the API\n",
            "Unless has no Synonyms in the API\n",
            "masterpeice has no Synonyms in the API\n",
            "masterpeice has no Synonyms in the API\n",
            "nowheresville has no Synonyms in the API\n",
            "-- has no Synonyms in the API\n",
            "Though has no Synonyms in the API\n",
            "Since has no Synonyms in the API\n",
            "Since has no Synonyms in the API\n",
            "Those has no Synonyms in the API\n",
            "... has no Synonyms in the API\n",
            "Bogdanovich has no Synonyms in the API\n",
            "... has no Synonyms in the API\n",
            "furiously has no Synonyms in the API\n",
            "furiously has no Synonyms in the API\n",
            "`` has no Synonyms in the API\n",
            "'' has no Synonyms in the API\n",
            "His has no Synonyms in the API\n",
            "Solondz has no Synonyms in the API\n",
            "Solondz has no Synonyms in the API\n",
            "-- has no Synonyms in the API\n",
            "`` has no Synonyms in the API\n",
            "`` has no Synonyms in the API\n",
            "`` has no Synonyms in the API\n",
            "-- has no Synonyms in the API\n",
            "Soderbergh has no Synonyms in the API\n",
            "-RRB- has no Synonyms in the API\n",
            "-LRB- has no Synonyms in the API\n",
            "-LRB- has no Synonyms in the API\n",
            "... has no Synonyms in the API\n",
            "masterfully has no Synonyms in the API\n",
            "masterfully has no Synonyms in the API\n",
            "historically has no Synonyms in the API\n",
            "masterfully has no Synonyms in the API\n",
            "What has no Synonyms in the API\n",
            "colorfully has no Synonyms in the API\n",
            "... has no Synonyms in the API\n",
            "Branagh has no Synonyms in the API\n",
            "500\n",
            "If has no Synonyms in the API\n",
            "inexplicably has no Synonyms in the API\n",
            "Britney has no Synonyms in the API\n",
            "Britney has no Synonyms in the API\n",
            "Kids has no Synonyms in the API\n",
            "`` has no Synonyms in the API\n",
            "Besco has no Synonyms in the API\n",
            "Norma has no Synonyms in the API\n",
            "Norma has no Synonyms in the API\n",
            "chillingly has no Synonyms in the API\n",
            "chillingly has no Synonyms in the API\n",
            "D.W. has no Synonyms in the API\n",
            "Kane has no Synonyms in the API\n",
            "Kane has no Synonyms in the API\n",
            "cluelessness has no Synonyms in the API\n",
            "filmmaking has no Synonyms in the API\n",
            "Freaks has no Synonyms in the API\n",
            "Freaks has no Synonyms in the API\n",
            "What has no Synonyms in the API\n",
            "Smarter has no Synonyms in the API\n",
            "Deuces has no Synonyms in the API\n",
            "Costner has no Synonyms in the API\n",
            "Costner has no Synonyms in the API\n",
            "... has no Synonyms in the API\n",
            "Oops has no Synonyms in the API\n",
            "`` has no Synonyms in the API\n",
            "Hugh has no Synonyms in the API\n",
            "Leaks has no Synonyms in the API\n",
            "Leaks has no Synonyms in the API\n",
            "-- has no Synonyms in the API\n",
            "-RRB- has no Synonyms in the API\n",
            "Steve has no Synonyms in the API\n",
            "Steve has no Synonyms in the API\n",
            "If has no Synonyms in the API\n",
            "-- has no Synonyms in the API\n",
            "600\n",
            "-- has no Synonyms in the API\n",
            "predictably has no Synonyms in the API\n",
            "predictably has no Synonyms in the API\n",
            "autocritique has no Synonyms in the API\n",
            "awkwardly has no Synonyms in the API\n",
            "Alternates has no Synonyms in the API\n",
            "-RRB- has no Synonyms in the API\n",
            "watchable has no Synonyms in the API\n",
            "watchable has no Synonyms in the API\n",
            "Solondz has no Synonyms in the API\n",
            "Rollerball has no Synonyms in the API\n",
            "Cusack has no Synonyms in the API\n",
            "Hanna-Barbera has no Synonyms in the API\n",
            "-- has no Synonyms in the API\n",
            "Frei has no Synonyms in the API\n",
            "What has no Synonyms in the API\n",
            "Frei has no Synonyms in the API\n",
            "Her has no Synonyms in the API\n",
            "Her has no Synonyms in the API\n",
            "Elvira has no Synonyms in the API\n",
            "If has no Synonyms in the API\n",
            "Salton has no Synonyms in the API\n",
            "modestly has no Synonyms in the API\n",
            "-- has no Synonyms in the API\n",
            "intermittently has no Synonyms in the API\n",
            "intermittently has no Synonyms in the API\n",
            "`` has no Synonyms in the API\n",
            "Salma has no Synonyms in the API\n",
            "Julie has no Synonyms in the API\n",
            "Nickleby has no Synonyms in the API\n",
            "... has no Synonyms in the API\n",
            "... has no Synonyms in the API\n",
            "` has no Synonyms in the API\n",
            "` has no Synonyms in the API\n",
            "Although has no Synonyms in the API\n",
            "... has no Synonyms in the API\n",
            "Ayurveda has no Synonyms in the API\n",
            "`` has no Synonyms in the API\n",
            "... has no Synonyms in the API\n",
            "directorial has no Synonyms in the API\n",
            "Zhang has no Synonyms in the API\n",
            "farawayaway has no Synonyms in the API\n",
            "sophomoric has no Synonyms in the API\n",
            "sophomoric has no Synonyms in the API\n",
            "These has no Synonyms in the API\n",
            "boldly has no Synonyms in the API\n",
            "boldly has no Synonyms in the API\n",
            "ou has no Synonyms in the API\n",
            "700\n",
            "` has no Synonyms in the API\n",
            "Cattaneo has no Synonyms in the API\n",
            "Nolan has no Synonyms in the API\n",
            "delightfully has no Synonyms in the API\n",
            "-- has no Synonyms in the API\n",
            "Craig has no Synonyms in the API\n",
            "Craig has no Synonyms in the API\n",
            "Provides has no Synonyms in the API\n",
            "We has no Synonyms in the API\n",
            "We has no Synonyms in the API\n",
            "incurably has no Synonyms in the API\n",
            "incurably has no Synonyms in the API\n",
            "'' has no Synonyms in the API\n",
            "'' has no Synonyms in the API\n",
            "Freaks has no Synonyms in the API\n",
            "`` has no Synonyms in the API\n",
            "Willie has no Synonyms in the API\n",
            "climactic has no Synonyms in the API\n",
            "-LRB- has no Synonyms in the API\n",
            "Reggio has no Synonyms in the API\n",
            "Godfrey has no Synonyms in the API\n",
            "Recoing has no Synonyms in the API\n",
            "Recoing has no Synonyms in the API\n",
            "Recoing has no Synonyms in the API\n",
            "-LRB- has no Synonyms in the API\n",
            "-LRB- has no Synonyms in the API\n",
            "-RRB- has no Synonyms in the API\n",
            "blazingly has no Synonyms in the API\n",
            "-LRB- has no Synonyms in the API\n",
            "Crimen has no Synonyms in the API\n",
            "del has no Synonyms in the API\n",
            "del has no Synonyms in the API\n",
            "` has no Synonyms in the API\n",
            "Yorker has no Synonyms in the API\n",
            "-- has no Synonyms in the API\n",
            "-- has no Synonyms in the API\n",
            "-LRB- has no Synonyms in the API\n",
            "Roger has no Synonyms in the API\n",
            "Roger has no Synonyms in the API\n",
            "Roger has no Synonyms in the API\n",
            "Michell has no Synonyms in the API\n",
            "McCrudden has no Synonyms in the API\n",
            "McCrudden has no Synonyms in the API\n",
            "Egoyan has no Synonyms in the API\n",
            "humanly has no Synonyms in the API\n",
            "humanly has no Synonyms in the API\n",
            "Without has no Synonyms in the API\n",
            "Without has no Synonyms in the API\n",
            "coulda has no Synonyms in the API\n",
            "800\n",
            "Wang has no Synonyms in the API\n",
            "This has no Synonyms in the API\n",
            "` has no Synonyms in the API\n",
            "` has no Synonyms in the API\n",
            "Movies has no Synonyms in the API\n",
            "Movies has no Synonyms in the API\n",
            "... has no Synonyms in the API\n",
            "When has no Synonyms in the API\n",
            "P.O.W. has no Synonyms in the API\n",
            "vs. has no Synonyms in the API\n",
            "vs. has no Synonyms in the API\n",
            "-- has no Synonyms in the API\n",
            "-- has no Synonyms in the API\n",
            "Greenfingers has no Synonyms in the API\n",
            "Fincher has no Synonyms in the API\n",
            "-LRB- has no Synonyms in the API\n",
            "-RRB- has no Synonyms in the API\n",
            "-RRB- has no Synonyms in the API\n",
            "Greenfingers has no Synonyms in the API\n",
            "-- has no Synonyms in the API\n",
            "This has no Synonyms in the API\n",
            "... has no Synonyms in the API\n",
            "Tomatoes has no Synonyms in the API\n",
            "Tomatoes has no Synonyms in the API\n",
            "Secrets has no Synonyms in the API\n",
            "Secrets has no Synonyms in the API\n",
            "Gentlemen has no Synonyms in the API\n",
            "This has no Synonyms in the API\n",
            "Romanek has no Synonyms in the API\n",
            "visuals has no Synonyms in the API\n",
            "Predictably has no Synonyms in the API\n",
            "Predictably has no Synonyms in the API\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQKQ2E1m4q6L"
      },
      "source": [
        "def random_swap(sentence, n=5): \r\n",
        "    #length = range(len(sentence)) \r\n",
        "    tokens = sentence.split(\" \")\r\n",
        "    length = len(tokens)\r\n",
        "    for _ in range(n):\r\n",
        "        idx1, idx2 = random.sample(range(length), 2)\r\n",
        "        tokens[idx1], tokens[idx2] = tokens[idx2], tokens[idx1] \r\n",
        "    return  (\" \").join(tokens)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYcuBbSR4yMk",
        "outputId": "d335b341-c617-4fcc-ab74-38213b455e82"
      },
      "source": [
        "for i in range(int(v_len)):\r\n",
        "  df_temp['sentence'][row] = random_swap(df_temp['sentence'][row])\r\n",
        "  \r\n",
        "  if (i%100 == 0):\r\n",
        "    print(i)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "2000\n",
            "2100\n",
            "2200\n",
            "2300\n",
            "2400\n",
            "2500\n",
            "2600\n",
            "2700\n",
            "2800\n",
            "2900\n",
            "3000\n",
            "3100\n",
            "3200\n",
            "3300\n",
            "3400\n",
            "3500\n",
            "3600\n",
            "3700\n",
            "3800\n",
            "3900\n",
            "4000\n",
            "4100\n",
            "4200\n",
            "4300\n",
            "4400\n",
            "4500\n",
            "4600\n",
            "4700\n",
            "4800\n",
            "4900\n",
            "5000\n",
            "5100\n",
            "5200\n",
            "5300\n",
            "5400\n",
            "5500\n",
            "5600\n",
            "5700\n",
            "5800\n",
            "5900\n",
            "6000\n",
            "6100\n",
            "6200\n",
            "6300\n",
            "6400\n",
            "6500\n",
            "6600\n",
            "6700\n",
            "6800\n",
            "6900\n",
            "7000\n",
            "7100\n",
            "7200\n",
            "7300\n",
            "7400\n",
            "7500\n",
            "7600\n",
            "7700\n",
            "7800\n",
            "7900\n",
            "8000\n",
            "8100\n",
            "8200\n",
            "8300\n",
            "8400\n",
            "8500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80d7GR9OYm9G",
        "outputId": "1c13ba10-274b-4847-a8db-22e35d0cc90d"
      },
      "source": [
        "df_train1 = df_train.append(df_temp)\r\n",
        "df_train1 = df_train1.reset_index(drop = True)\r\n",
        "df_train1.shape\r\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17088, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ6o_79ISSVb"
      },
      "source": [
        "## Defining Fields"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e63g08ijOrf7"
      },
      "source": [
        "Now we shall be defining LABEL as a LabelField, which is a subclass of Field that sets sequen tial to False (as it’s our numerical category class). TWEET is a standard Field object, where we have decided to use the spaCy tokenizer and convert all the text to lower‐ case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qk8IP4SK1Lrp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e554e82-4d18-4d78-fe60-92a299d65671"
      },
      "source": [
        "# Import Library\n",
        "import random\n",
        "import torch, torchtext\n",
        "from torchtext import data \n",
        "\n",
        "# Manual Seed\n",
        "SEED = 43\n",
        "torch.manual_seed(SEED)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f5edd510e28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6bKQax2Mf_U"
      },
      "source": [
        "Tweet = data.Field(sequential = True, tokenize = 'spacy', batch_first =True, include_lengths=True)\n",
        "Label = data.LabelField(tokenize ='spacy', is_target=True, batch_first =True, sequential =False)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX-lYIe_O7Vy"
      },
      "source": [
        "Having defined those fields, we now need to produce a list that maps them onto the list of rows that are in the CSV:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VawdWq36O6td"
      },
      "source": [
        "fields = [('sentence', Tweet),('labels',Label)]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbtZ-Ph2P1xL"
      },
      "source": [
        "Armed with our declared fields, lets convert from pandas to list to torchtext. We could also use TabularDataset to apply that definition to the CSV directly but showing an alternative approach too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3OLcJ5B7rHz"
      },
      "source": [
        "example_train = [data.Example.fromlist([df_train1.sentence[i],df_train1.labels[i]], fields) for i in range(df_train1.shape[0])] \r\n",
        "example_valid = [data.Example.fromlist([df_valid.sentence[i],df_valid.labels[i]], fields) for i in range(df_valid.shape[0])] "
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT-flpH-P1cd"
      },
      "source": [
        "# Creating dataset\n",
        "#twitterDataset = data.TabularDataset(path=\"tweets.csv\", format=\"CSV\", fields=fields, skip_header=True)\n",
        "\n",
        "#twitterDataset = data.Dataset(example, fields)\n",
        "\n",
        "train = data.Dataset(example_train, fields)\n",
        "valid = data.Dataset(example_valid, fields)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6ZnyCPaR08F"
      },
      "source": [
        "Finally, we can split into training, testing, and validation sets by using the split() method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPYXyuKhRpBk"
      },
      "source": [
        "#(train, valid) = twitterDataset.split(split_ratio=[0.85, 0.15], random_state=random.seed(SEED))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykvsCGQMR6UD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5685927-793b-422c-ba3f-00d376b2d6a6"
      },
      "source": [
        "(len(train), len(valid))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17088, 1101)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kix8P2IKSBaV"
      },
      "source": [
        "An example from the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUpEOQruR9JL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d962bd0-bec3-4c7d-c9ca-0c98a9121d84"
      },
      "source": [
        "vars(train.examples[10])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'labels': 1,\n",
              " 'sentence': ['Good',\n",
              "  'fun',\n",
              "  ',',\n",
              "  'good',\n",
              "  'action',\n",
              "  ',',\n",
              "  'good',\n",
              "  'acting',\n",
              "  ',',\n",
              "  'good',\n",
              "  'dialogue',\n",
              "  ',',\n",
              "  'good',\n",
              "  'pace',\n",
              "  ',',\n",
              "  'good',\n",
              "  'cinematography',\n",
              "  '.']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKdllP3FST4N"
      },
      "source": [
        "## Building Vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuvWQ-SpSmSz"
      },
      "source": [
        "At this point we would have built a one-hot encoding of each word that is present in the dataset—a rather tedious process. Thankfully, torchtext will do this for us, and will also allow a max_size parameter to be passed in to limit the vocabu‐ lary to the most common words. This is normally done to prevent the construction of a huge, memory-hungry model. We don’t want our GPUs too overwhelmed, after all. \n",
        "\n",
        "Let’s limit the vocabulary to a maximum of 5000 words in our training set:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx955u93SGeY"
      },
      "source": [
        "Tweet.build_vocab(train)\n",
        "Label.build_vocab(train)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvyEeEjXTGhX"
      },
      "source": [
        "By default, torchtext will add two more special tokens, <unk> for unknown words and <pad>, a padding token that will be used to pad all our text to roughly the same size to help with efficient batching on the GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA3tIESdcJdN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "298a9ed9-c612-45b1-8054-27b2a5940cb1"
      },
      "source": [
        "print('Size of input vocab : ', len(Tweet.vocab))\n",
        "print('Size of label vocab : ', len(Label.vocab))\n",
        "print('Top 10 words appreared repeatedly :', list(Tweet.vocab.freqs.most_common(10)))\n",
        "print('Labels : ', Label.vocab.stoi)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of input vocab :  17907\n",
            "Size of label vocab :  5\n",
            "Top 10 words appreared repeatedly : [('.', 16081), (',', 14262), ('the', 12181), ('and', 8951), ('of', 8904), ('a', 8852), ('to', 6059), ('-', 5455), (\"'s\", 5093), ('is', 5080)]\n",
            "Labels :  defaultdict(<function _default_unk_index at 0x7f5edac1a048>, {2: 0, 1: 1, 3: 2, 4: 3, 0: 4})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwjD2-ebTeUX"
      },
      "source": [
        "**Lots of stopwords!!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLWW221gTpNs"
      },
      "source": [
        "Now we need to create a data loader to feed into our training loop. Torchtext provides the BucketIterator method that will produce what it calls a Batch, which is almost, but not quite, like the data loader we used on images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQqMhMoDUDmn"
      },
      "source": [
        "But at first declare the device we are using."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zfo2QhGJUK4l"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LK0FcLRZ4Hzq"
      },
      "source": [
        ""
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK2ORoqdTNsM"
      },
      "source": [
        "train_iterator, valid_iterator = data.BucketIterator.splits((train, valid), batch_size = 32, \n",
        "                                                            sort_key = lambda x: len(x.sentence),\n",
        "                                                            sort_within_batch=True, device = device)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gg7gTFQO4fby"
      },
      "source": [
        "Save the vocabulary for later use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niE9Cc6-2bD_"
      },
      "source": [
        "import os, pickle\n",
        "with open('tokenizer.pkl', 'wb') as tokens: \n",
        "    pickle.dump(Tweet.vocab.stoi, tokens)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AbsQwqkVyAy"
      },
      "source": [
        "## Defining Our Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4PED4HJWH4t"
      },
      "source": [
        "We use the Embedding and LSTM modules in PyTorch to build a simple model for classifying tweets.\n",
        "\n",
        "In this model we create three layers. \n",
        "1. First, the words in our tweets are pushed into an Embedding layer, which we have established as a 300-dimensional vector embedding. \n",
        "2. That’s then fed into a 2 stacked-LSTMs with 100 hidden features (again, we’re compressing down from the 300-dimensional input like we did with images). We are using 2 LSTMs for using the dropout.\n",
        "3. Finally, the output of the LSTM (the final hidden state after processing the incoming tweet) is pushed through a standard fully connected layer with three outputs to correspond to our three possible classes (negative, positive, or neutral)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43pVRccMT0bT"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class classifier(nn.Module):\n",
        "    \n",
        "    # Define all the layers used in model\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout):\n",
        "        \n",
        "        super().__init__()          \n",
        "        \n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        # LSTM layer\n",
        "        self.encoder = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           dropout=dropout,\n",
        "                           bidirectional = True,\n",
        "                           batch_first=True)\n",
        "        # try using nn.GRU or nn.RNN here and compare their performances\n",
        "        # try bidirectional and compare their performances\n",
        "        \n",
        "        # Dense layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "        # text = [batch size, sent_length]\n",
        "        embedded = self.embedding(text)\n",
        "        # embedded = [batch size, sent_len, emb dim]\n",
        "      \n",
        "        # packed sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True)\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.encoder(packed_embedded)\n",
        "        #hidden = [batch size, num layers * num directions,hid dim]\n",
        "        #cell = [batch size, num layers * num directions,hid dim]\n",
        "    \n",
        "        # Hidden = [batch size, hid dim * num directions]\n",
        "        hidden = self.dropout(hidden[-1,:,:])\n",
        "        dense_outputs = self.fc(hidden)   \n",
        "        \n",
        "        # Final activation function softmax\n",
        "        output = F.softmax(dense_outputs, dim=1)\n",
        "            \n",
        "        return output"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwBoGE_X_Fl8"
      },
      "source": [
        "# Define hyperparameters\n",
        "size_of_vocab = len(Tweet.vocab)\n",
        "embedding_dim = 300\n",
        "num_hidden_nodes = 100\n",
        "num_output_nodes = v_range\n",
        "num_layers = 3\n",
        "dropout = 0.5\n",
        "\n",
        "# Instantiate the model\n",
        "model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes, num_output_nodes, num_layers, dropout = dropout)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-pOMqzJ3eTv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c134eb9-cb2e-4b68-a519-eb645ca38ef8"
      },
      "source": [
        "print(model)\n",
        "\n",
        "#No. of trianable parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classifier(\n",
            "  (embedding): Embedding(17907, 300)\n",
            "  (encoder): LSTM(300, 100, num_layers=3, batch_first=True, dropout=0.5, bidirectional=True)\n",
            "  (fc): Linear(in_features=100, out_features=5, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n",
            "The model has 6,177,405 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXajorf5Xz7t"
      },
      "source": [
        "## Model Training and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrE9RpMtZ1Vs"
      },
      "source": [
        "First define the optimizer and loss functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u86JWdlXvu5"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# define optimizer and loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# define metric\n",
        "def binary_accuracy(preds, y):\n",
        "    #round predictions to the closest integer\n",
        "    _, predictions = torch.max(preds, 1)\n",
        "    \n",
        "    correct = (predictions == y).float() \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "    \n",
        "# push to cuda if available\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VCJtNb3Zt8w"
      },
      "source": [
        "The main thing to be aware of in this new training loop is that we have to reference `batch.tweets` and `batch.labels` to get the particular fields we’re interested in; they don’t fall out quite as nicely from the enumerator as they do in torchvision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WjEPLKsAiS_"
      },
      "source": [
        "**Training Loop**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDWNnGK3Y5oJ"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    # initialize every epoch \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    # set the model in training phase\n",
        "    model.train()  \n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        # resets the gradients after every batch\n",
        "        optimizer.zero_grad()   \n",
        "        \n",
        "        # retrieve text and no. of words\n",
        "        tweet, tweet_lengths = batch.sentence   \n",
        "        \n",
        "        # convert to 1D tensor\n",
        "        predictions = model(tweet, tweet_lengths).squeeze()  \n",
        "        \n",
        "        # compute the loss\n",
        "        loss = criterion(predictions, batch.labels)        \n",
        "        \n",
        "        # compute the binary accuracy\n",
        "        acc = binary_accuracy(predictions, batch.labels)   \n",
        "        \n",
        "        # backpropage the loss and compute the gradients\n",
        "        loss.backward()       \n",
        "        \n",
        "        # update the weights\n",
        "        optimizer.step()      \n",
        "        \n",
        "        # loss and accuracy\n",
        "        epoch_loss += loss.item()  \n",
        "        epoch_acc += acc.item()    \n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZcHhkkvAsCt"
      },
      "source": [
        "**Evaluation Loop**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHEe-zSVAriL"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    # initialize every epoch\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    # deactivating dropout layers\n",
        "    model.eval()\n",
        "    \n",
        "    # deactivates autograd\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "        \n",
        "            # retrieve text and no. of words\n",
        "            tweet, tweet_lengths = batch.sentence\n",
        "            \n",
        "            # convert to 1d tensor\n",
        "            predictions = model(tweet, tweet_lengths).squeeze()\n",
        "            \n",
        "            # compute loss and accuracy\n",
        "            loss = criterion(predictions, batch.labels)\n",
        "            acc = binary_accuracy(predictions, batch.labels)\n",
        "            \n",
        "            # keep track of loss and accuracy\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6LJFW7HaJoV"
      },
      "source": [
        "**Let's Train and Evaluate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tq330XlnaEU9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f43a1116-bd2f-49f7-cd50-5d57fe30e0c4"
      },
      "source": [
        "N_EPOCHS = 30\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "     \n",
        "    # train the model\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    \n",
        "    # evaluate the model\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    # save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% \\n')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 1.380 | Train Acc: 53.43%\n",
            "\t Val. Loss: 1.451 |  Val. Acc: 45.43% \n",
            "\n",
            "\tTrain Loss: 1.369 | Train Acc: 53.63%\n",
            "\t Val. Loss: 1.451 |  Val. Acc: 45.43% \n",
            "\n",
            "\tTrain Loss: 1.368 | Train Acc: 53.68%\n",
            "\t Val. Loss: 1.451 |  Val. Acc: 45.34% \n",
            "\n",
            "\tTrain Loss: 1.366 | Train Acc: 53.82%\n",
            "\t Val. Loss: 1.451 |  Val. Acc: 45.25% \n",
            "\n",
            "\tTrain Loss: 1.365 | Train Acc: 53.95%\n",
            "\t Val. Loss: 1.451 |  Val. Acc: 45.43% \n",
            "\n",
            "\tTrain Loss: 1.364 | Train Acc: 54.09%\n",
            "\t Val. Loss: 1.450 |  Val. Acc: 45.70% \n",
            "\n",
            "\tTrain Loss: 1.363 | Train Acc: 54.19%\n",
            "\t Val. Loss: 1.451 |  Val. Acc: 45.43% \n",
            "\n",
            "\tTrain Loss: 1.361 | Train Acc: 54.39%\n",
            "\t Val. Loss: 1.451 |  Val. Acc: 45.25% \n",
            "\n",
            "\tTrain Loss: 1.359 | Train Acc: 54.55%\n",
            "\t Val. Loss: 1.452 |  Val. Acc: 45.25% \n",
            "\n",
            "\tTrain Loss: 1.357 | Train Acc: 54.76%\n",
            "\t Val. Loss: 1.454 |  Val. Acc: 44.99% \n",
            "\n",
            "\tTrain Loss: 1.355 | Train Acc: 55.03%\n",
            "\t Val. Loss: 1.455 |  Val. Acc: 44.99% \n",
            "\n",
            "\tTrain Loss: 1.355 | Train Acc: 54.93%\n",
            "\t Val. Loss: 1.452 |  Val. Acc: 45.25% \n",
            "\n",
            "\tTrain Loss: 1.354 | Train Acc: 55.02%\n",
            "\t Val. Loss: 1.455 |  Val. Acc: 44.99% \n",
            "\n",
            "\tTrain Loss: 1.353 | Train Acc: 55.19%\n",
            "\t Val. Loss: 1.453 |  Val. Acc: 45.16% \n",
            "\n",
            "\tTrain Loss: 1.352 | Train Acc: 55.28%\n",
            "\t Val. Loss: 1.454 |  Val. Acc: 44.99% \n",
            "\n",
            "\tTrain Loss: 1.350 | Train Acc: 55.45%\n",
            "\t Val. Loss: 1.455 |  Val. Acc: 44.99% \n",
            "\n",
            "\tTrain Loss: 1.350 | Train Acc: 55.48%\n",
            "\t Val. Loss: 1.454 |  Val. Acc: 45.08% \n",
            "\n",
            "\tTrain Loss: 1.348 | Train Acc: 55.68%\n",
            "\t Val. Loss: 1.451 |  Val. Acc: 45.43% \n",
            "\n",
            "\tTrain Loss: 1.347 | Train Acc: 55.74%\n",
            "\t Val. Loss: 1.461 |  Val. Acc: 44.36% \n",
            "\n",
            "\tTrain Loss: 1.346 | Train Acc: 55.85%\n",
            "\t Val. Loss: 1.453 |  Val. Acc: 45.16% \n",
            "\n",
            "\tTrain Loss: 1.345 | Train Acc: 55.95%\n",
            "\t Val. Loss: 1.451 |  Val. Acc: 45.34% \n",
            "\n",
            "\tTrain Loss: 1.345 | Train Acc: 55.94%\n",
            "\t Val. Loss: 1.454 |  Val. Acc: 45.08% \n",
            "\n",
            "\tTrain Loss: 1.344 | Train Acc: 56.04%\n",
            "\t Val. Loss: 1.454 |  Val. Acc: 45.08% \n",
            "\n",
            "\tTrain Loss: 1.344 | Train Acc: 56.07%\n",
            "\t Val. Loss: 1.453 |  Val. Acc: 45.16% \n",
            "\n",
            "\tTrain Loss: 1.344 | Train Acc: 56.13%\n",
            "\t Val. Loss: 1.452 |  Val. Acc: 45.25% \n",
            "\n",
            "\tTrain Loss: 1.343 | Train Acc: 56.21%\n",
            "\t Val. Loss: 1.454 |  Val. Acc: 45.08% \n",
            "\n",
            "\tTrain Loss: 1.342 | Train Acc: 56.23%\n",
            "\t Val. Loss: 1.454 |  Val. Acc: 45.08% \n",
            "\n",
            "\tTrain Loss: 1.342 | Train Acc: 56.31%\n",
            "\t Val. Loss: 1.456 |  Val. Acc: 44.90% \n",
            "\n",
            "\tTrain Loss: 1.341 | Train Acc: 56.41%\n",
            "\t Val. Loss: 1.457 |  Val. Acc: 44.63% \n",
            "\n",
            "\tTrain Loss: 1.340 | Train Acc: 56.48%\n",
            "\t Val. Loss: 1.463 |  Val. Acc: 44.09% \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZgzB0ZkHVTI"
      },
      "source": [
        "## Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZZfnWo0abRx"
      },
      "source": [
        "#load weights and tokenizer\n",
        "\n",
        "path='./saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path));\n",
        "model.eval();\n",
        "tokenizer_file = open('./tokenizer.pkl', 'rb')\n",
        "tokenizer = pickle.load(tokenizer_file)\n",
        "\n",
        "#inference \n",
        "\n",
        "def classify_tweet(tweet):\n",
        "    \n",
        "    categories = {0: \"Negative\", 1:\"Positive\", 2:\"Neutral\"}\n",
        "    \n",
        "    # tokenize the tweet \n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(tweet)] \n",
        "    # convert to integer sequence using predefined tokenizer dictionary\n",
        "    indexed = [tokenizer[t] for t in tokenized]        \n",
        "    # compute no. of words        \n",
        "    length = [len(indexed)]\n",
        "    # convert to tensor                                    \n",
        "    tensor = torch.LongTensor(indexed).to(device)   \n",
        "    # reshape in form of batch, no. of words           \n",
        "    tensor = tensor.unsqueeze(1).T  \n",
        "    # convert to tensor                          \n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    # Get the model prediction                  \n",
        "    prediction = model(tensor, length_tensor)\n",
        "\n",
        "    _, pred = torch.max(prediction, 1) \n",
        "    \n",
        "    return categories[pred.item()]"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTkHLEipIlM9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0c8f9242-b2f3-4335-9415-596077d20d18"
      },
      "source": [
        "classify_tweet(\"A valid explanation for why Trump won't let women on the golf course.\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Negative'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVjCuKK_LVEF"
      },
      "source": [
        "## Discussion on Data Augmentation Techniques \n",
        "\n",
        "You might wonder exactly how you can augment text data. After all, you can’t really flip it horizontally as you can an image! :D \n",
        "\n",
        "In contrast to data augmentation in images, augmentation techniques on data is very specific to final product you are building. As its general usage on any type of textual data doesn't provides a significant performance boost, that's why unlike torchvision, torchtext doesn’t offer a augmentation pipeline. Due to powerful models as transformers, augmentation tecnhiques are not so preferred now-a-days. But its better to know about some techniques with text that will provide your model with a little more information for training. \n",
        "\n",
        "### Synonym Replacement\n",
        "\n",
        "First, you could replace words in the sentence with synonyms, like so:\n",
        "\n",
        "    The dog slept on the mat\n",
        "\n",
        "could become\n",
        "\n",
        "    The dog slept on the rug\n",
        "\n",
        "Aside from the dog's insistence that a rug is much softer than a mat, the meaning of the sentence hasn’t changed. But mat and rug will be mapped to different indices in the vocabulary, so the model will learn that the two sentences map to the same label, and hopefully that there’s a connection between those two words, as everything else in the sentences is the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_uEfWJpL6Nq"
      },
      "source": [
        "### Random Insertion\n",
        "A random insertion technique looks at a sentence and then randomly inserts synonyms of existing non-stopwords into the sentence n times. Assuming you have a way of getting a synonym of a word and a way of eliminating stopwords (common words such as and, it, the, etc.), shown, but not implemented, in this function via get_synonyms() and get_stopwords(), an implementation of this would be as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Alm5D7WIvAC"
      },
      "source": [
        "def remove_mystopwords(sentence):\n",
        "    tokens = sentence.split(\" \")\n",
        "    tokens_filtered= [word for word in text_tokens if not word in my_stopwords]\n",
        "    return (\" \").join(tokens_filtered)\n",
        "\n",
        "def random_insertion(sentence, n): \n",
        "    words = remove_stopwords(sentence) \n",
        "    for _ in range(n):\n",
        "        new_synonym = get_synonyms(random.choice(words))\n",
        "        sentence.insert(randrange(len(sentence)+1), new_synonym) \n",
        "    return sentence"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqLWzwJ3Mm8h"
      },
      "source": [
        "## Random Deletion\n",
        "As the name suggests, random deletion deletes words from a sentence. Given a probability parameter p, it will go through the sentence and decide whether to delete a word or not based on that random probability. Consider of it as pixel dropouts while treating images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7Dz7JJfMqyC"
      },
      "source": [
        "def random_deletion(words, p=0.5): \n",
        "    if len(words) == 1: # return if single word\n",
        "        return words\n",
        "    remaining = list(filter(lambda x: random.uniform(0,1) > p,words)) \n",
        "    if len(remaining) == 0: # if not left, sample a random word\n",
        "        return [random.choice(words)] \n",
        "    else:\n",
        "        return remaining"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOIbi5WzO5OU"
      },
      "source": [
        "### Random Swap\n",
        "The random swap augmentation takes a sentence and then swaps words within it n times, with each iteration working on the previously swapped sentence. Here we sample two random numbers based on the length of the sentence, and then just keep swapping until we hit n."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdnTyHS83_zB",
        "outputId": "1964eb0c-fe51-4c24-a731-9c1fe7747fe7"
      },
      "source": [
        "random.sample(range(200), 2)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[102, 78]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnkbG15HO3Yj"
      },
      "source": [
        "def random_swap(sentence, n=5): \n",
        "    #length = range(len(sentence)) \n",
        "    tokens = sentence.split(\" \")\n",
        "    length = len(tokens)\n",
        "    for _ in range(n):\n",
        "        idx1, idx2 = random.sample(range(length), 2)\n",
        "        tokens[idx1], tokens[idx2] = tokens[idx2], tokens[idx1] \n",
        "    return  (\" \").join(tokens)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "096Ciu993Mkz",
        "outputId": "d6b5ac82-088c-4475-b2a3-f5727420af1e"
      },
      "source": [
        "print(random_swap(\"I am testing swapping\"))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I am swapping testing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "599NpwfMR5Vm"
      },
      "source": [
        "For more on this please go through this [paper](https://arxiv.org/pdf/1901.11196.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5aeKuNCRGip"
      },
      "source": [
        "### Back Translation\n",
        "\n",
        "Another popular approach for augmenting text datasets is back translation. This involves translating a sentence from our target language into one or more other languages and then translating all of them back to the original language. We can use the Python library googletrans for this purpose. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHhNBbYrRXNy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22bac3c5-d518-4c32-9b13-3b5d423ec649"
      },
      "source": [
        "import random\n",
        "import google_trans_new\n",
        "from google_trans_new import google_translator \n",
        "\n",
        "translator = google_translator ()\n",
        "sentence = ['The dog slept on the rug']\n",
        "\n",
        "available_langs = list(google_trans_new.LANGUAGES.keys()) \n",
        "trans_lang = random.choice(available_langs) \n",
        "print(f\"Translating to {google_trans_new.LANGUAGES[trans_lang]}\")\n",
        "\n",
        "translations = translator.translate(sentence, lang_tgt=trans_lang) \n",
        "print(translations)\n",
        "\n",
        "\n",
        "translations_en_random = translator.translate(translations, lang_src=trans_lang, lang_tgt='en') \n",
        "print(translations_en_random)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Translating to yoruba\n",
            "['Ajá náà sùn sórí àkéte'] \n",
            "['The dog is lying on the bed'] \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}